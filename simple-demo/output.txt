## 4. Methodology

### 4.1 System Methodology & Data Processing Workflow

Our implemented system employs a dual-mode architecture with separate processing pipelines for audio and video that synchronize through event-based coordination. This approach provides greater flexibility and improved fault tolerance compared to a unified pipeline. The methodology consists of the following components and processes:

#### 1. Data Acquisition and Processing Architecture

The system features two distinct operational modes, each with its own data acquisition and processing workflow:

**Practice Speaking Mode:**
- Captures continuous video from webcam and audio from microphone
- Video frames are processed independently from audio to optimize performance
- Audio is processed in chunks of configurable duration (typically 5-10 seconds)
- Processing results are synchronized through state management and event listeners

**Mock Interview Mode:**
- Adds an AI interviewer created through ElevenLabs Conversation API
- Enables two-way communication with turn-taking between user and AI
- Maintains synchronized transcript with role identification
- Collects comprehensive session data for post-interview evaluation

#### 2. Emotion Detection Implementation

The emotion classification system uses face-api.js, a JavaScript implementation of convolutional neural networks optimized for in-browser execution:

```javascript
// Model loading and initialization
await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
await faceapi.nets.faceExpressionNet.loadFromUri('/models');

// Emotion detection algorithm
async detectEmotions() {
  // Process video frame at reduced resolution for performance
  const detections = await faceapi.detectAllFaces(
    this.videoElement, 
    new faceapi.TinyFaceDetectorOptions()
  ).withFaceExpressions();
  
  // Extract highest confidence emotion
  if (detections.length > 0) {
    const expressions = detections[0].expressions;
    const emotion = Object.keys(expressions).reduce((a, b) => 
      expressions[a] > expressions[b] ? a : b
    );
    return { emotion, confidence: expressions[emotion] };
  }
  return null;
}
```

Performance optimization was achieved through:
- Asynchronous processing of video frames on a throttled interval (2000ms default)
- Guard clauses to prevent concurrent processing of multiple frames
- Reduced resolution processing to minimize resource consumption

#### 3. Audio Processing Pipeline

The audio processing system implements a multi-stage pipeline:

**Recording and Chunking:**
- Audio is captured using the Web Audio API's MediaRecorder
- Recorded in chunks of configurable duration
- Chunks are encoded as WAV format for compatibility

**Transcription Processing:**
- Audio chunks are sent to ElevenLabs Speech-to-Text API
- The API returns transcribed text with confidence scores
- Transcriptions are matched with emotion data based on timestamps

**Role-based Transcript Collection:**
```javascript
// Speaker role identification and storage
function addToTranscript(text, role) {
  transcriptData.push({
    role: role,  // 'user' or 'interviewer'
    text: text,
    timestamp: new Date().toISOString(),
    emotion: currentEmotion // Captured from emotion detector
  });
  
  // Update UI with new transcript entry
  renderTranscriptEntry(text, role, currentEmotion);
}
```

#### 4. Interview Agent Implementation

The mock interview functionality is powered by ElevenLabs Conversation API with custom agent configuration:

**Agent Creation and Configuration:**
```javascript
// Agent initialization with dynamic parameters
async function createInterviewAgent(params) {
  const agentConfig = {
    name: `${params.role} Interviewer`,
    description: `An interviewer for ${params.company} conducting a ${params.role} interview`,
    instructions: generateInterviewInstructions(params),
    voice_id: "selected_voice_id",
    // Additional configuration parameters
  };
  
  return await apiClient.createAgent(agentConfig);
}
```

**Conversation Management:**
```javascript
// WebSocket connection using official SDK
import { Conversation } from '@11labs/client';

// Session initialization with event handling
const conversation = await Conversation.startSession({
  agentId: agentId,
  onConnect: handleConnectionEvent,
  onMessage: (message) => {
    // Process incoming message
    processInterviewerResponse(message.text);
    
    // Add to transcript with role identification
    transcriptData.push({
      role: 'interviewer',
      text: message.text,
      timestamp: new Date().toISOString()
    });
  },
  onDisconnect: handleDisconnectEvent
});
```

#### 5. Multimodal Fusion Strategy

Rather than implementing a unified pipeline with continuous batching as initially proposed, we implemented an event-based synchronization approach:

**Temporal Alignment:**
- Emotion data and transcribed text are matched based on timestamps
- Each transcript entry is associated with the most recent emotion detection
- This loose coupling allows independent processing while maintaining context

**State Management:**
```javascript
// Example state update with emotion context
function updateApplicationState(newTranscriptEntry, currentEmotion) {
  // Add emotion context to transcript entry
  newTranscriptEntry.emotion = currentEmotion;
  
  // Update global application state
  appState.transcript.push(newTranscriptEntry);
  appState.currentEmotion = currentEmotion;
  
  // Trigger UI updates
  renderApplication();
}
```

#### 6. Report Generation and Evaluation

The interview evaluation process implements a structured analysis approach:

**Data Collection:**
- Complete transcript with speaker roles and timestamps
- Emotion data throughout the interview session
- Interview parameters (role, company, duration)

**GPT-based Evaluation Algorithm:**
```javascript
// Report generation with comprehensive prompt engineering
async function generateInterviewReport(transcriptData, emotionData) {
  // Construct context-rich prompt with transcript and emotions
  const prompt = constructEvaluationPrompt(transcriptData, emotionData);
  
  // Generate evaluation using OpenAI API
  const response = await openaiClient.createCompletion({
    model: "gpt-4o-mini",
    prompt: prompt,
    max_tokens: 1500,
    temperature: 0.7
  });
  
  // Process and structure the response
  return processEvaluationResponse(response.choices[0].text);
}
```

**Report Structuring:**
- Evaluation is divided into sections (strengths, weaknesses, recommendations)
- Markdown formatting is applied for improved readability
- The report includes both content analysis and emotional response assessment

#### 7. System Performance Optimization

Several optimization techniques were implemented to ensure real-time performance:

**Emotion Detection Optimization:**
- Throttled processing of video frames (default 2-second intervals)
- Guard clauses to prevent concurrent processing
- Reduced resolution for more efficient neural network inference

**Network Optimization:**
- CORS handling through Vite proxy configuration
- Client-side caching of models to reduce network load
- Compression of audio data before transmission

**Memory Management:**
- Proper cleanup of resources during mode switching
- Garbage collection triggers for large objects
- Event listener cleanup to prevent memory leaks

The implemented methodology demonstrated that a browser-based multimodal AI system can achieve sufficient performance for real-time interaction while maintaining accuracy in emotion detection and speech processing. The dual-pipeline approach with event-based synchronization proved more robust than the initially proposed unified pipeline, offering better fault tolerance and flexibility.
